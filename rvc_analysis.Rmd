---
title: "rvc_analysis"
author: "Ben Best"
date: "January 18, 2017"
output:
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(raster)
library(sp)
library(leaflet)
library(vegan)
select = dplyr::select
```

## Introduction

Already fetched RVC data from ERDDAP server in erddap_rvc.Rmd for 1999 - 2014.

_**Units**_: **density** # of individuals observed over 5 minute period within a 15 m diameter circle (and depth column)

## Summarize Species Densities to Mapgrid Locations

- vignette: [Diversity analysis in vegan](https://cran.r-project.org/web/packages/vegan/vignettes/diversity-vegan.pdf)

- Lefcheck (2015) [The use of functional traits to elucidate
the causes and consequences of biological diversity](http://www.vims.edu/library/Theses/Lefcheck15.pdf)

- Lefcheck (2014) [Dimensions of biodiversity in Chesapeake Bay demersal fishes: patterns and drivers through space and time](http://onlinelibrary.wiley.com/doi/10.1890/ES13-00284.1/abstract) with supplemental [R script](https://figshare.com/articles/Supplement_1_R_script_containing_all_data_analyses_and_functional_phylogenetic_and_taxonomic_trees_in_Newick_format_/3563847)

- [Lab 8. Communities](http://benbestphd.com/landscape-ecology-labs/lab8.html) from [BB's Landscape Ecology Labs/](http://benbestphd.com/landscape-ecology-labs/) ([github](https://github.com/bbest/landscape-ecology-labs))

    - Schick et al (2011) [Community structure in pelagic marine mammals at large spatial scales](http://www.int-res.com/abstracts/meps/v434/p165-181/) _MEPS_
  
    ![](./img/Schick-et-al2011_Fig4-ord-env-contours.png)
  

### Diversity indices

Copied from `vegan` vignette...

Function `diversity()` finds the most commonly used diversity
indices (Hill, 1973):

$$
H = - \sum_{i=1}^S p_i \log_b p_i  \text{Shannon--Weaver}\\
$$

$$
D_1 = 1 - \\sum_{i=1}^S p_i^2        &\\text{Simpson}\\
D_2 = \\frac{1}{\\sum_{i=1}^S p_i^2} &\\text{inverse Simpson}\\
$$

where $p_i$ is the proportion of species $i$, and $S$ is the number of
species so that $\sum_{i=1}^S p_i = 1$, and $b$ is the base of the
logarithm.  It is most common to use natural logarithms (and then we
mark index as $H'$), but $b=2$ has
theoretical justification. The default is to use natural logarithms.
Shannon index is calculated with:
<<>>=
H <- diversity(BCI)
@
which finds diversity indices for all sites.

\pkg{Vegan} does not have indices for evenness (equitability), but
the most common of these, Pielou's evenness $J = H'/\log(S)$ is easily
found as:
<<>>=
J <- H/log(specnumber(BCI))
@
where `specnumber} is a simple \pkg{vegan} function to find
the numbers of species.

\pkg{vegan} also can estimate series of R\'{e}nyi and Tsallis
diversities. R{\'e}nyi diversity of order $a$ is \citep{Hill73number}:
\begin{equation}
H_a = \frac{1}{1-a} \log \sum_{i=1}^S p_i^a \,,
\end{equation}
and the corresponding Hill number is $N_a = \exp(H_a)$.  Many common
diversity indices are special cases of Hill numbers: $N_0 = S$, $N_1 =
\exp(H')$, $N_2 = D_2$, and $N_\infty = 1/(\max p_i)$. The
corresponding R\'{e}nyi diversities are $H_0 = \log(S)$, $H_1 = H'$, $H_2 =
- \log(\sum p_i^2)$, and $H_\infty = - \log(\max p_i)$.  
Tsallis diversity of order $q$ is \citep{Tothmeresz95}:
\begin{equation}
  H_q = \frac{1}{q-1} \left(1 - \sum_{i=1}^S p^q \right) \, .
\end{equation}
These correspond to common diversity indices: $H_0 = S-1$, $H_1 = H'$,
and $H_2 = D_1$, and can be converted to Hill numbers:
\begin{equation}
  N_q = (1 - (q-1) H_q )^\frac{1}{1-q} \, .
\end{equation}


## Fetching Data

```{r fetch}

```

## Summarize by Mapgrid & Species

Fetched data had the following nested elements:

- count (`quantificationValue`) of species (`scientificName`)
- species size classes (`observedMeanLengthInCm`)
- secondary station number (`station_nr`)
- primary sampling unit (`primarySamplingUnit`)
- map grid number (`mapGridNumber`)
- year of sampling event (`eventDate`)

Next, we aggregate to having average density of species per map grid.

```{r subset, eval=T}
use_subset = T
rvc_rds     = 'data/rvc.rds'
rvc_mg_csv  = 'data/rvc_mapgrid_locations.csv'
rvc_spp_csv = 'data/rvc_species_densities.csv'
  

if ( !all(file.exists(c(rvc_mg_csv, rvc_spp_csv))) ){
  
  # read data fetched from erddap (erddap_rvc.Rmd)
  rvc = read_rds(rvc_rds)
    
  # summarize individual species counts to PSU
  rvc_psu = rvc %>%
    select(datasetID, eventDate, mapGridNumber, primarySamplingUnit, station_nr, 
           scientificName, quantificationValue) %>%
    filter(!is.na(station_nr)) %>%
    group_by(datasetID, eventDate, mapGridNumber, primarySamplingUnit, scientificName) %>%
    summarise(
      q = sum(quantificationValue, na.rm=T) / length(unique(station_nr))) %>%
    filter(q > 0, !is.na(q)) # filter: NA, 0's
  
  # mapgrid locations
  rvc_mg = rvc %>%
    group_by(datasetID, mapGridNumber) %>% # 200m x 200m
    summarise(
      latitude = mean(latitude, na.rm=T), 
      longitude = mean(longitude, na.rm=T))

  # summarize species to mapgrid locations
  rvc_spp = rvc_psu %>%
    left_join(rvc_mg_loc, by=c('datasetID','mapGridNumber')) %>%
    mutate(
      year = year(eventDate)) %>%
    group_by(year, datasetID,  mapGridNumber, scientificName) %>%
    summarize(
      q_mean = mean(q))
```

## Normalize

Do we normalize species density by site across species, species across sites, or a combination of both? Eg using max. Do the biodiversity metrics already do this? Eg, `vegdist()` applies `wisconsin()` normalization.

- Normalize within the species, or across the number of all individuals?
$$
n_s / max(n_s) \\
n_s / max(N_S) \\
$$
```{r normalize}
  # summarize by species
   rvc_mg %>%
     group_by(scientificName, year) %>%
     summarise(
       mg_q_var  = var(q_mean),
       mg_q_mean = mean(q_mean)) %>%
     ungroup()
dnorm = vegdist(rvc_mg, method="bray", binary=F, diag=F, upper=F) 
```
## Biodiversity Metrics
```{r vegan prep}
# vegan prep
  rvc_mg_wide = rvc_mg %>%
    ungroup() %>%
    spread(scientificName, q_mean, fill=0)

```
- **Richness**
  - `specnumber`, 
  - `rarefy`: rarefied species richness for community ecologists. Normalizes by effort using rarefaction curves.
```{r Richness}  
  richness= rarefy(rvc_mg_wide,)
  rcurve= rarecurve()
```  
- **Evenness**: 
  - `Simpson` 1/D (1 = completely even)
  - `Shannon`, effective: rewards higher species linearly (vs just Shannon)

```{r Effective Simpson}
  rvc_mg_eff_simpson = 1/(1-diversity(rvc_mg_wide %>% select(-year, -datasetID, -mapGridNumber), index='simpson'))
``` 
```{r Effective Shannon}
    rvc_mg_eff_shannon = exp(diversity(rvc_mg_wide %>% select(-year, -datasetID, -mapGridNumber), index='shannon'))
```

Spatial variants:
- **alpha**
- **gamma**
- **beta**

Others:
- **functional diversity**
```{Effective Rao's Q}
#read in species list 
spname= readr::read_csv(species_code_species_common_names.rdm)
#read in trait*species dataframe 
sptrait= readr::read_csv(species_trait_matrix.csv)
#read in species*community relative abundance dataframe 

#compute relative abundance of each species in each year and community
#remove 'sp.' in abundance dataframe - identified to family 
#substitute scientific name with species common name 

#arrange alphabetically 

#match species in trait dataframe with species in abundance dataframe 

#compute Gowers distance correcting with Podani for ordered traits 
traits.dist=gowdis(traits,ord="podani")

#Because values of Rao's Q can be maximized when fewer than the max number of functional types are present unless distances are ultramtetric ---- Clustering method to produce ultrametric dendrogram 

#to account for sensitivity in clustering use multiple algorithms  (Mouchet et al., 2008) 
tree_methods = c("single","complete","average","mcquitty","ward")
trees=lapply(tree_methods,function(i) hclust(traits.dist,method=i))
par(mfrow=c(3,2))
for(i in 1:length(trees)) {plot(trees[[i]])}

#convert trees to ultrametric
trees.ultra=lapply(trees,function(i) cl_ultrametric(as.hclust(i)))

#Plot each tree
par(mfrow=c(3,2))
for (i in 1:length(trees.ultra)) {plot(trees.ultra[[i]])}

#Build the consensus tree (Mouchet et al 2008 Oikos)
ensemble.trees=cl_ensemble(list=trees)
class(ensemble.trees)
consensus.tree=cl_consensus(ensemble.trees) 
#plot(consensus.tree)

#Calculate dissimilarity values for each tree using 2-norm (Merigot et al 2010 Ecology) to determine which tree best preserves orignial distances
all.trees=c(trees.ultra,consensus.tree[1])
names(all.trees)=c(tree_methods,"consensus")
(trees.dissim=lapply(all.trees,function(i) cl_dissimilarity(i,traits.dist,method="spectral")))

#Identify best tree and isolate
trees.dissim2=do.call(rbind,trees.dissim)
min.tree=which.min(trees.dissim2)
names(all.trees)[min.tree]
func.dist=all.trees[names(all.trees)==names(all.trees)[min.tree]][[1]]

#Confirm lowest 2-norm value
cl_dissimilarity(func.dist,traits.dist,method="spectral")

#Scale by the max value so that all values are between 0-1 (clue package)
func.dist=func.dist/max(func.dist)
#Plot the best tree
# par(mfrow=c(1,1))
# par(mar=c(3,1,0,16))
# plot(func.dist,horiz=TRUE)
#Save plot: 10" x 15"

#Write newick tree
write.tree(as.phylo(as.hclust(func.dist)),"Functional dendrogram")

#calculate effective Rao's Q 
#Calculate alpha diversity
# TODO: change below inputs to just relative abundance 
alphadiv.list=lapply(list(catch_abund,catch_biomass,catch_pres.abs),
 function(i) {
   #Extract community matrix from object in list
   mat=i[,c(1,18:(ncol(i)-1))]
   rownames(mat)=mat[,1]; mat=mat[,-1]
   #Calculate relative values for community matrix
   rel.mat=mat/apply(mat,1,sum)
   #Compute functional diversity 
   func.div=1/(1-apply(rel.mat,1,function(x) t(x) %*% as.matrix(func.dist) %*% x))
 }
```
- **phylogenetic diversity**

##Correlation
```{r correlation}

#Test significance of Spearman rank correlations (H0 that rho = 0)
do.call(rbind,lapply(list("richness","evenness","simpson","shannon", "func.div"),function(j) {
  do.call(rbind,lapply(rev(list("richness","evenness","simpson","shannon","func.div")),function(k) {
    data.frame(title=paste(j,k,sep="~"),
               correlation=cor.test(alphadiv.list[[2]][,j],alphadiv.list[[2]][,k])$estimate,
               p.value=cor.test(alphadiv.list[[2]][,j],alphadiv.list[[2]][,k])$p.value) } ) ) } ) ) 

```

BB TODO:
- Translate [Lab 8. Communities](http://benbestphd.com/landscape-ecology-labs/lab8.html) for calculating Bray-Curtis dissimilarity, and applying NMDS & clustering with environmental gradients (gam contour plot). 

## Scaling up to Abundance

Can we use known habitat distributions (and possibly other environmental data) to predict abundance of a species for the same extent as known stock assessments?

Scaling up to **abundance**:

$$
d_s = n_s / a_s \\
a_s = \pi * 7.5 ^2 \\
n_A = d_s * A / a_s
$$

```
  esimp_d = rvc_mg_wide %>%
    select(year, datasetID, mapGridNumber) %>%
    mutate(
      eff_simpson = rvc_mg_eff_simpson) %>%
    left_join(
      rvc_mg_loc,
      by=c('datasetID','mapGridNumber'))
  
  esimp_pts = esimp_d
  coordinates(esimp_pts) = ~longitude+latitude
  
  # plot simpson for one year
  spplot(esimp_pts %>% subset(year==1999), zcol='eff_simpson')
  
  
  # interactive plot
  pal = colorNumeric('Spectral', esimp_pts@data$eff_simpson)
  
  esimp_pts %>%
    subset(year==2000) %>%
    leaflet() %>% 
      addProviderTiles('Esri.OceanBasemap') %>%
      addCircleMarkers(
        radius = 10,
        color = ~pal(eff_simpson),
        stroke = FALSE, fillOpacity = 0.5)
  
  
  # hex plot
  ggplot(esimp_d, aes(x=longitude, y=latitude, color=eff_simpson)) +
    geom_hex() + 
    scale_fill_distiller('Eff Simp', palette='Spectral')
  
  library(trelliscope)
  
  
  # columns ignored (for now):
  # basisOfRecord,bottomType,class,country,datasetID,datasetName,depth,dynamicProperties,eventDateRemarks,eventDateTimeZone,family,genus,geodeticDatum,habitat,habitat_cd,higherInstitutionCode,institutionCode,kingdom,latitude,locality,longitude,mapNumber,materialSampleID,maximumDepthInMeters,minimumDepthInMeters,observedMeanLengthInCm,order,ownerInstitutionCode,phylum,protection,quantificationMethod,quantificationName,quantificationStatus,quantificationUnit,,recordedBy,region,sampleRadiusInMeters,Samples,scientificNameAuthorship,scientificNameIDITIS,scientificNameIDWoRMS,species_cd,species_nr,specificEpithet,stateProvince,subregion_nr,taxonRank,time,timeUncertainty,underwaterVisibilityInMeters,vernacularName,verticalDatum,waterBody,waterTemperatureInCelsius,zone_nr
  
```

```{r analysis old, eval=F, echo=F}

d %>%
  mutate(
    year = year(eventDate)) %>%
  group_by(primarySamplingUnit, year) %>% # TODO: group_by(..., MPA)
  # confirm randomly stratified, so each primarySamplingUnit has one date
  # summarize(
  #   n_dates = length(unique(eventDate))) %>%
  # select(primarySamplingUnit, year, n_dates)
  select(primarySamplingUnit, year, station_nr, eventDate) %>%
  arrange(primarySamplingUnit, year, eventDate, station_nr) %>%
  View()
  summarize(
    
    # n_psu = length(unique(primarySamplingUnit))) %>%
    # select(year, n_psu) %>%
    # arrange(n_psu)

   n_station = length(unique(station_nr))) %>% #station_nr is station number? 
    select(year, n_station) %>%
    arrange(n_station)

#find if any unique psu has all zeros for 
  filter(quantificationValue > 0, !is.na(quantificationValue)) %>% # subset: 11,011 -> 1,529
  dim()

```

## Relating to the Environment

### Use Values in Dataset

Existing values:

- bottomType:
- mapNumber: in/out of MPA
- depth: bottom or in water column where observation?
  - minimumDepthInMeters
  - maximumDepthInMeters
- underwaterVisibilityInMeters: use that as a filter, and offset to account for bias
- waterTemperatureInCelsius
- habitat, habitat_cd: Artificial Reef, Continuous High/Medium/Low Relief, Isolated High/Medium/Low Relief (Patch)
- zone_nr: like Inshore, Foreef, Lagoon...

So we can evaluate the relationship between these predictors for basic understanding. But to predict elsewhere, we need these data for locations outside survey dataset.


### Extract and Predict with Outside Values

- BB asked Maria for seascapes data to extract and predict with this dataset (2017-01-19)


## Background

- **Ecological Reserves** larger and intention of protecting fisheries

- **SPA** (sanctuary preservation area) for dive spots, small

```{r functional diversity} 



## readr::read_csv() # keeps strings as character, vs evil factors. yay! from readr
# utils::read.csv() # evil! base read of csv
sp_traits = traits %>%
  as_tibble() %>%
  select(-c(1:3)) %>%      # remove extra columns
  arrange(Common_name) %>% # arrange alphabetically by species
  filter(!Common_name == 'Na#') %>%
  mutate( 
    # factors are EVIL!
    Common_name = tolower(as.character(Common_name)),
    # change misclassified categorical traits to numeric 
    Maxlength     = as.numeric(as.character(Maxlength)),
    Trophic_level = as.numeric(as.character(Trophic_level)),
    # ordinal traits
    Complexity   = ordered(
      Complexity, levels=c("Low","Medium","High")),
      Gregariousness = ordered(Gregariousness, levels=c("1","2","3"))) %>%
  as.data.frame()

# conform to: species x traits
rownames(sp_traits) = sp_traits$Common_name
sp_traits = sp_traits %>% select(-Common_name)
head(sp_traits)

sp_sr_ra$data_wide[[1]]


#### OLD (Jan 18)
#Calculate Gower distances
traits.dist = gowdis(traits, ord="podani")

#Functional Diversity
sp_sr_ra %>%
  mutate(
    fd = map(data_wide, dbFD)
  )

# remove species rows in traits without abundances
sp_traits = sp_traits[rownames(sp_traits) %in% colnames(sp_sr_ra$data_wide[[1]]),]

traits_dist = gowdis(sp_traits, ord="podani")

dim(sp_traits)
dim(sp_sr_ra$data_wide[[1]])

#functional_div = dbFD(sp_traits, sp_sr_ra$data_wide[[1]])
#functional_div = dbFD(traits_dist, sp_sr_ra$data_wide[[1]])
# worked w/out factors...
functional_div = dbFD(
  sp_traits %>%
    select(Maxlength, Trophic_level), 
  sp_sr_ra$data_wide[[1]])

# didn't work: Species x species distance matrix was still is not Euclidean after 'sqrt' correction. Use another correction method.
# functional_div = dbFD(
#   sp_traits %>%
#     select(Maxlength, Trophic_level, Water_column), 
#   sp_sr_ra$data_wide[[1]])

# didn't work: Species x species distance matrix was still is not Euclidean after 'sqrt' correction. Use another correction method.
traits_dist = gowdis(
  sp_traits %>%
    select(Maxlength, Trophic_level, Water_column), ord="podani",)

functional_div = dbFD(traits_dist, sp_sr_ra$data_wide[[1]],corr = 'cailliez', calc.FRic= F, calc.FGR=F, calc.CWM=F )

```


## TODO

- Where's the Dry Tortugas data, eg for 2000?
- check out MPA

- fix [obis_biodiv.Rmd](https://marinebon.github.io/analysis/obis_biodiv.html) with proper names

## Notes for ERDDAP Correction

- `mapNumber` -> `mpaNumber`
- `Samples`: 361,351 records with Samples # and all other fields are NA or NaN?!

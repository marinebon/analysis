---
title: "RVC_analysis"
author: "Ben Best"
date: "January 18, 2017"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=T)

#rm(list = ls())

suppressPackageStartupMessages({
  library(tidyverse)
  library(lubridate)
  library(raster)
  library(sp)
  library(leaflet)
  library(scales)
  library(vegan)
  library(rerddap)
  library(DT)
  library(readxl)
  library(stringr)
  library(FD)
  library(clue)
  library(fpc) #Calls: pamk
  library(psych)
  library(raster)
  #library(dygraphs)
  #library(xts)
  library(rareNMtesets) #sample-based rarefaction curve 
})

select = dplyr::select
```

## Fetch RVC Data for FK and DY

* Combined ERDDAP datasets across years for Florida Keys and Dry Tortugas Reef Visual Census
* Fetched RVC data from ERDDAP server in erddap_rvc.Rmd for 1999 - 2016.

[ERDDAP: gcoos4.tamu.edu](http://gcoos4.tamu.edu:8080/erddap/info/index.html?page=1&itemsPerPage=1000)

Fetched data had the following nested elements:
* count (`quantificationValue`) of species (`scientific_name`)
* species size classes (`observedMeanLengthInCm`)
* secondary station number (`station_nr`)
* primary sampling unit (`primarySamplingUnit`)
* map grid number (`mapGridNumber`)
* year of sampling event (`eventDate`)
* protection level (`protection`)

```{r Fetch RVC data}
# csv files to read (since time consuming to create using ERDDAP server calls)
rvc_rds     = 'data/rvc.rds'
rvc_mg_csv  = 'data/rvc_mapgrid_locations.csv'
rvc_spp_csv = 'data/rvc_species_densities.csv'

# assign ERDDAP server URL
eurl = 'http://gcoos4.tamu.edu:8080/erddap/' # search/index.html?page=1&itemsPerPage=1000&searchFor=Florida+Keys+Reef+Fish+Visual+Census

# only load full rvc.rds if need be (since slow)
if (!all(file.exists(c(rvc_mg_csv, rvc_spp_csv)))){
  if (!file.exists(rvc_rds)){
    # create csv files
    
    # search for datasets
    ed_search(query='Reef Visual Census', which='table', url=eurl)
    # TODO / NOTE: ed_search(page_size, page) DO NOT WORK! ACK!
    #ed_search(query='Reef Visual Census', which='table', url=eurl, page=2)
  
    # iterate over years
    all = list()
    
    ids = c(
      # Dry Tortugas
      sprintf('dt%d', c(1999,2000,2004,2006,2008,2010,2012,2014,2016)),
      # Florida Keys
      sprintf('fk%d', 1999:2014)) # diff't method 1994:1998
    
    for (id in ids){ # id = ids[1]
    
      # construct id for dataset
      csv_id   = sprintf('data/%s.csv', id)
      csv_vars = sprintf('data/%s_vars.csv', id)
      cat(sprintf('%s\n', id))
      
      if (!file.exists(csv_vars)){
        # get metadata for dataset
        m = try(info(id, url=eurl), silent = T)
        
        if (class(m) == 'try-error') next
        cat(sprintf('  writing %s\n', csv_vars))
        write_csv(m$variables, csv_vars)
      }
      
      if (!file.exists(csv_id)){
      
        # get metadata for dataset
        m = try(info(id, url=eurl), silent = T)
        
        # if id not found, then move onto next year
        if (class(m) == 'try-error'){
          cat(sprintf('  %s NOT FOUND!\n', id)) # fk2013 NOT FOUND! b/c not there
          next
        } 
        
        # try fetching data, up to 10x
        dap_attempts = 1
        while (dap_attempts < 11){
          
          # load data for individual dataset
          cat('  fetching data', ifelse(dap_attempts > 1, sprintf(': attempt %d\n', dap_attempts), '\n'))
          d_id = try(tabledap(m, fields=m$variables$variable_name, url=eurl), silent = T)
          
          # break out of loop if not giving error
          if (!'try-error' %in% class(d_id)) break
          dap_attempts =+ 1
          cat('  error fetching data\n')
        }
      
        # write to csv
        cat('  writing to csv\n')
        d_id %>%
          tbl_df() %>%
          write_csv(csv_id)
        
      } # end if (!file.exists(csv_id))
    } # end for (yr in 1994:2014)
    
    # bind all csv's into one data frame
    cat("bind all csv's\n")
    d = data_frame()
    for (f in list.files('data','[fk|dt][0-9]+\\.csv', full.names=T)){
      cat(' ', f,'\n')
      d_f = read_csv(
        f, progress=F, trim_ws=T,
        col_types = cols(
          protection = col_character()))
      d = bind_rows(d, d_f)
    }
    
    write_rds(d, rvc_rds) # 4.4 GB
  
    # evaluate commonness of columns
    cat('eval common columns')
    vars = data_frame()
    for (f in list.files('data', '[fk|dt][0-9]+_vars\\.csv', full.names = T)){ # f = list.files('data', 'fk[0-9]+_vars\\.csv', full.names = T)[1]
      cat(f)
  
      vars = bind_rows(
        vars,
        read_csv(f) %>%
          mutate(
            id = str_replace(f, 'data/(.*)_vars\\.csv', '\\1')))
    }
    
    v = vars %>%
      select(variable_name, data_type, id) %>% # drop actual_range
      spread(id, data_type)
    
  
  } else {
    # read data
    d = read_rds(rvc_rds)
  }
  
  # show columns (with same class) not consistently available across all datasets
  summary(d)
  
  d %>%
    head() %>%
    datatable()
}
#View(fk$sample_data)
```

## Summarize by Mapgrid & Species

Next, we aggregate to having average density of species from Secondary Sampling Unit (SSU) and Primary Sampling Unit (PSU) to Map grid (200x200m).

```{r aggregate}
if ( !all(file.exists(c(rvc_mg_csv, rvc_spp_csv))) ){
  
  # read data fetched from erddap (erddap_rvc.Rmd)
  rvc = read_rds(rvc_rds)
    
  # summarize individual species counts to PSU
  rvc_psu = rvc %>%
    select(datasetID, eventDate, mapGridNumber, primarySamplingUnit, station_nr, protection,
           scientificName, quantificationValue) %>%
    filter(!is.na(station_nr)) %>%
    group_by(datasetID, eventDate, mapGridNumber, primarySamplingUnit, protection, scientificName) %>%
    summarise(
      n_stations = length(unique(station_nr)),
      q = sum(quantificationValue, na.rm=T) / length(unique(station_nr))) %>%
    filter(q > 0, !is.na(q)) # filter: NA, 0's
  
  # mapgrid locations
  rvc_mg = rvc %>%
    group_by(datasetID, mapGridNumber) %>% # 200m x 200m
    summarise(
      latitude = mean(latitude, na.rm=T), 
      longitude = mean(longitude, na.rm=T))

  # summarize species to mapgrid locations
  rvc_spp = rvc_psu %>%
    left_join(rvc_mg, by=c('datasetID','mapGridNumber')) %>%
    mutate(
      year = year(eventDate)) %>%
    group_by(year, datasetID,  mapGridNumber, protection, scientificName) %>%
    summarize(
      q_mean = mean(q)) %>%
    ungroup()
  
  # add sampling effort per datasetID (with year) and mapGridNumber by n_stations (secondary sampling units)
  rvc_spp = rvc_spp %>%
    left_join(
      rvc_psu %>%
        group_by(datasetID, eventDate, mapGridNumber, primarySamplingUnit, n_stations) %>%
        summarize(n_spp = n()) %>%
        group_by(datasetID, mapGridNumber) %>%
        summarize(
          n_stations = sum(n_stations)), 
      by = c('datasetID', 'mapGridNumber'))
      
  # write to csv
  write_csv(rvc_mg, rvc_mg_csv)
  write_csv(rvc_spp, rvc_spp_csv)
}

# read in data
rvc_mg = read_csv(rvc_mg_csv)
rvc_spp = read_csv(rvc_spp_csv)
```

## Filter RVC data by level of Protection

1. By protection (Inside and outside MPA) [protection]
    1. Sanctuary Preservation Areas (SPA)
        + Alligator Reef Sanctuary Preservation Area
        + Carysfort Sanctuary Preservation Area
        + Cheeca Rocks Sanctuary Preservation Area
        + Coffins Patch Sanctuary Preservation Area
        + Conch Reef Sanctuary Preservation Area
        + Davis Reef Sanctuary Preservation Area
        + Eastern Dry Rocks Sanctuary Preservation Area
        + The Elbow Sanctuary Preservation Area
        + French Reef Sanctuary Preservation Area
        + Grecian Rocks Sanctuary Preservation Area
        + Hen and Chickens Sanctuary Preservation Area
        + Key Largo Dry Rocks Sanctuary Preservation Area
        + Looe Key Sanctuary Preservation Area
        + Molasses Reef Sanctuary Preservation Area
        + Newfound Harbor Key Sanctuary Preservation Area
        + Sand Key Sanctuary Preservation Area
        + Rock Key Sanctuary Preservation Area
        + Sombrero Key Sanctuary Preservation Area
    1. Ecological Reserve (ER)
        + Tortugas Ecological Reserve
        + Western Sambo Ecological Reserve
    1. Special Use Areas (SU)
        + Conch Reef Research Only Area
        + Eastern Sambo Research Only Area
        + Looe Key Research Only Area
        + Tennesse Reef Research Only Area
        
```{r RVC filter by MPA}

```

## Filter RVC data by subregions 

1.  By subregions [latitude,longitude]
    1. Upper Keys 
        + latitude  >24.95
    1. Middle Keys
        + latitude  >24.63 and <= 24.95
        + longitude >-81.10 and <= -80.45
    1. Lower Keys
        + latitude  >24.55 and <= 24.63
        + longitude >-82.65 and <=-81.10
    1. Dry Tortugas 
        + latitude  >24.55 and <= 24.75
        + longitude >-83.5 and <=-82.65

```{r RVC filtered by subregions}

```

## species detectability 
[Buckland et al., 2011. Biodiversity monitoring: the relevance of detectability. Biological diversity: frontiers in measurement and assessment.] 
Some species are inherently more easily detected than others. 
The RVC data was designed to detect exploited reef fish and does not do as well at detecting cryptic species (e.g. lionfish, gobies, blennies)
If detectability is not estimated, then biodiversity will be biased. However ignoring detectability might not be a major problem if the bias is consistant across time and space 
$$ 
\hat{p_i} = \hat{N_i}/\sum_{i=1}^S N_i = estimatedproportion of species i in the regoin \\

N_i = n_i/\hat\pi_i = estimated number of indiviudals of species i in the region\\

n_i = number of individuals of species i detected in the region \\

\hat\pi_i = probability of detection of an individual of species i 
$$
```{r Account for species detectability}

```

```{r wide}
rvc_spp = read_csv(rvc_spp_csv) %>%
  select(
    year, dataset_id = datasetID, 
    mapgridnum = mapGridNumber, scientific_name = scientificName, 
    q_mean, n_stations)

nstations_df = function(x){
  group_by(x, dataset_id, mapgridnum) %>% summarize(n_stations = first(n_stations))
}

#vegan prep
rvc_tbl = rvc_spp %>%
  nest(-year) %>%
  mutate(
    data_wide  = map(data, ~ spread(data=.x, scientific_name, q_mean, fill=0)))
```

## Biodiversity Metrics

- **Richness**:
  - `Richness`, total number of species 
```{r Richness}  
rvc_tbl = rvc_tbl %>%
  mutate(
    richness = map(data_wide, function(x) specnumber(x %>% select(-dataset_id, -mapgridnum, -n_stations))))

hist(rvc_tbl$richness[[1]])
```
 - `Sample-based rarefaction`
 
```{r Sample-based rarefaction}  

```
- **Species Accumulation Model**:   
  - `Arrhenius species-area model`
  $$
  S = cX^z
  $$
Where X is the area (size) of the patch or site, and c and z are parameters. Parameter c is uninteresting,
but z gives the steepness of the species area curve and is a measure of beta diversity.
```{r  Arrhenius species–area model}

``` 
- **Evenness**: 
  - `Inverse Simpson Diversity`, 1/D (1 = completely even)
  - `Gini-Simpson Diversity`, 1-D 
```{r Effective Simpson Diversity, eval=FALSE}
# Effective Simpson of the entire FKNMS 
rvc_tbl = rvc_tbl %>%
  mutate(
    eff_simpson = map(
      data_wide, 
      function(x) 1 / (1 - diversity(x %>% select(-dataset_id, -mapgridnum, -n_stations), index='simpson'))),
    eff_simpson_bray = map(
      veg_bray_m, 
      function(x) 1 / (1 - diversity(x %>% select(-dataset_id, -mapgridnum, -n_stations), index='simpson')))) 
boxplot(rvc_tbl$eff_simpson)
boxplot(rvc_tbl$eff_simpson_bray)


rvc_tbl$data_wide[[1]]
rvc_tbl$veg_bray[1]

hist(rvc_tbl$eff_simpson[[1]])
#glimpse(rvc_tbl)


``` 
  - `Shannon Diversity Index`, effective: rewards higher species linearly (vs just Shannon)
```{r Effective Shannon}

rvc_tbl = rvc_tbl %>%
  mutate(
    eff_shannon = map(data_wide, function(x) exp(diversity(x %>% select(-dataset_id, -mapgridnum, -n_stations), index='shannon'))))

#hist(rvc_tbl$eff_shannon[[1]])
#boxplot(mean(rvc_tbl$eff_simpson))

```
  - `Pielous Evenness`,
```{r Pielous Evenness}

rvc_tbl = rvc_tbl %>%
  mutate(
    evenness = map(data_wide, function(x) diversity(x %>% select(-dataset_id, -mapgridnum, -n_stations), index='shannon')/log(specnumber(x))))

#hist(rvc_tbl$evenness[[1]])

```
- **Functional Diversity**
Functional Dendogram from Lefcheck (2014) [Dimensions of biodiversity in Chesapeake Bay demersal fishes: patterns and drivers through space and time](http://onlinelibrary.wiley.com/doi/10.1890/ES13-00284.1/abstract) with supplemental [R script](https://figshare.com/articles/Supplement_1_R_script_containing_all_data_analyses_and_functional_phylogenetic_and_taxonomic_trees_in_Newick_format_/3563847)

```{r Functional Dendogram}

# wide: common_name x traits
d_traits = rvc_grp %>% 
  group_by(
    common_name, maxlength, trophic_level, trophic_group, water_column, diel_activity, substrate_type, complexity, gregariousness) %>%
  summarize(
    n = n()) %>%
  select(-n) %>%
  ungroup() %>%
  mutate(
    # ordinal traits
    complexity = factor(
      complexity, levels=c("Low","Medium","High"), ordered=T)) %>%
  arrange(common_name) %>%
  as.data.frame()

#dim(d_traits) 333, 8 (species*traits)

#d_traits
View(d_traits)

# conform to: species x traits
rownames(d_traits) = d_traits$common_name
d_traits = select(d_traits, -common_name)
head(d_traits)

#Calculate Gower distances
traits.dist=gowdis(d_traits,ord="podani")

#Use clustering method to produce ultrametric dendrogramBecause values of Rao's Q can be maximized when fewer than the max number of functional types are present unless distances are ultramtetric 

#to account for sensitivity in clustering use multiple algorithms  (Mouchet et al., 2008) 
tree_methods = c("single","complete","average","mcquitty","ward")
trees=lapply(tree_methods,function(i) hclust(traits.dist, method=i))
par(mfrow=c(3,2))
for(i in 1:length(trees)) {plot(trees[[i]])}

#convert trees to ultrametric
trees.ultra=lapply(trees,function(i) cl_ultrametric(as.hclust(i)))

#Plot each tree
par(mfrow=c(3,2))
for (i in 1:length(trees.ultra)) {plot(trees.ultra[[i]])}

#Build the consensus tree (Mouchet et al 2008 Oikos) from package clue 
ensemble.trees=cl_ensemble(list=trees) #list of clusterings 
class(ensemble.trees)
consensus.tree=cl_consensus(ensemble.trees) #synthesizes the information in the elements of a cluster ensemble into a single clustering 
plot(consensus.tree)

#Calculate dissimilarity values for each tree using 2-norm (Merigot et al 2010 Ecology) to determine which tree best preserves orignial distances
all.trees=c(trees.ultra,consensus.tree[1])
names(all.trees)=c(tree_methods,"consensus")
(trees.dissim=lapply(all.trees,function(i) cl_dissimilarity(i,traits.dist,method="spectral"))) #spectral norm (2-norm) of the differences of the ultrametrics

#Identify best tree and isolate
trees.dissim2=do.call(rbind,trees.dissim)
min.tree=which.min(trees.dissim2)
names(all.trees)[min.tree]
func.dist=all.trees[names(all.trees)==names(all.trees)[min.tree]][[1]]

#Confirm lowest 2-norm value
cl_dissimilarity(func.dist,traits.dist,method="spectral")

#Scale by the max value so that all values are between 0-1 (clue package)
func.dist=func.dist/max(func.dist)

#Plot the best tree
par(mfrow=c(1,1))
par(mar=c(3,1,0,16))
plot(func.dist,horiz=TRUE)
#Save plot: 10" x 15"

#Write newick tree
write.tree(as.phylo(as.hclust(func.dist)),"Functional dendrogram")
```

Rao's Q

```{r Effective Functional Diversity}
rvc_grp = read_csv('data/rvc_spp_grouped.csv') # View(rvc_grp)

# wide: mapgridnum x common_name -> q_mean
w_mcq = rvc_grp %>% 
  arrange(common_name) %>%
  select(year, dataset_id, mapgridnum, common_name, q_mean) %>%
  # NOTE: spread() before nest() by year gives max columns, vs fewer if after
  spread(common_name, q_mean, fill=0) %>% # View() # dim() 336 columns
  group_by(year) %>%
  nest() %>%
  as.data.frame()

#calculate effective Rao's Q 
function(i){
  func_div=1/(1-apply(w_mcq,1, function(x) t(x) %*% as.matrix(func.dist)%*% x))}
rvc_grp = read_csv('data/rvc_spp_grouped.csv') # View(rvc_grp)

```

## Correlation

```{r correlation}

```

## Form Functional Groups based on Trophic Level and Importance 

```{r traits to groups}
# read in species traits
spp_traits = read_csv('data/species_trait_matrix.csv') %>%
  mutate(
    Substrate_type = recode(
      Substrate_type, 
      'Hard bottoml' = 'Hard bottom',
      'Hardbottom' = 'Hard bottom'))

# remove white space on values and fix column names
spp_traits = apply(spp_traits, 2, function(x) str_trim(x)) %>%
  as_tibble()
names(spp_traits) = names(spp_traits) %>%
  str_trim() %>% str_replace(' ', '_') %>% tolower()
#View(spp_traits)

# read in species roles
spp_roles = read_excel(
  'data/Habitat_Indicator_Dataset_Metric_Hepner.xlsx',
  'Species Ecological Role') 

# remove white space on values and fix column names
spp_roles = apply(spp_roles, 2, function(x) str_trim(x)) %>%
  as_tibble()
names(spp_roles) = names(spp_roles) %>%
  str_trim() %>% str_replace(' ', '_') %>% tolower()
#View(spp_roles)
 
spp_roles = spp_roles %>%
  mutate(
    habitat  = zoo::na.locf(habitat), # fill down habitat
    importance = recode(
      importance, 
      'exploited, focal'       = 'exploited', 
      'exploited, charismatic' = 'exploited'),
    importance = ifelse(
      str_detect(indicator, '.*angelfish') & importance == 'ecologically important',
      'ornamental', importance)) %>%
  filter(!is.na(indicator))
#View(spp_roles)

# join to rvc_spp
rvc_grp = rvc_spp %>%
  select(
    year, dataset_id = datasetID, 
    mapgridnum = mapGridNumber, scientific_name = scientificName, q_mean, n_stations) %>%
  left_join(
    spp_traits,
    by = c('scientific_name'='latin_name')) %>%
  left_join(
    spp_roles,
    by = c('scientific_name'='scientific_name')) %>%
  mutate(
    group = ifelse(is.na(importance), trophic_group, importance))
# View(rvc_grp)

rvc_sum = rvc_grp %>%
  group_by(importance, trophic_group, group, scientific_name, common_name) %>%
  summarize(
    n = n())
write_csv(rvc_sum, 'data/rvc_spp_group-summary.csv')
rvc_sum %>%
  filter(is.na(group)) %>%
  write_csv('data/rvc_spp_no-group-removed.csv')

rvc_grp = rvc_grp %>%
  filter(!is.na(group))

write_csv(rvc_grp, 'data/rvc_spp_grouped.csv')
```  

```{r rvc_grp_sum.csv}
rvc_grp = read_csv('data/rvc_spp_grouped.csv') # View(rvc_grp)

rvc_grp_sum = rvc_grp %>%
  group_by(group, year) %>%
  summarize(
    q_mean = mean(q_mean, na.rm=T),
    n_species = length(unique(scientific_name))) %>%
  ungroup() # View(rvc_grp_sum)

write_csv(rvc_grp_sum, 'data/rvc_grp_years.csv')
```

```{r crosstalk map of spp richness markers}
library(tidyverse)
library(leaflet)
library(scales)
library(DT)
library(crosstalk)

rvc_spp = read_csv('data/rvc_spp_grouped.csv')
rvc_mg  = read_csv('data/rvc_mapgrid_locations.csv')

spp_rich = rvc_spp %>%
  group_by(year,dataset_id, mapgridnum) %>%
  summarize(
    n_spp = length(unique(scientific_name))) %>%
  left_join(
    rvc_mg,
    by=c('dataset_id'='datasetID', 'mapgridnum'='mapGridNumber')) %>%
  rename(lat=latitude, lon=longitude)


# # Wrap data frame in SharedData
# sd <- SharedData$new(spp_rich[sample(nrow(spp_rich), 100),])
# 
# # Create a filter input
# filter_slider(
#   "year", "Year", sd, column=~year, step=1, width=with(sd$data(), max(year) - min(year)))
# 
# # Use SharedData like a dataframe with Crosstalk-enabled widgets
# bscols(
#   leaflet(sd) %>% 
#     addTiles() %>% 
#     addMarkers(lat=~lat, lng=~lon),
#   datatable(sd, extensions="Scroller", style="bootstrap", class="compact", width="100%",
#     options=list(deferRender=TRUE, scrollY=300, scroller=TRUE))
# )


# Wrap data frame in SharedData
#sd <- SharedData$new(spp_rich[sample(nrow(spp_rich), 100),])
sd <- SharedData$new(spp_rich[sample(nrow(spp_rich), 100),])
sd <- SharedData$new(spp_rich) # View(sd$data())

# Create a filter input
#filter_slider(
#   "year", "Year", sd, column=~date) # , width='90%', animate=T, timeFormat='%Y')
filter_slider(
  "year", "Year", sd, column=~year, width='90%', animate=T, sep='')
#, step=2, dragRange = 2)
  
  #sep='')
#with(sd$data(), max(year) - min(year))

# Create a palette that maps factor levels to colors
pal <- colorNumeric('YlOrRd', sd$data()$n_spp) # 'Spectral'

# Use SharedData like a dataframe with Crosstalk-enabled widgets
bscols(
  leaflet(sd) %>% 
    addProviderTiles('Esri.OceanBasemap') %>%
    addCircleMarkers(
      #lng = ~lon, lat = ~lat,
      radius = ~rescale(n_spp, to=c(0.1, 10)),
      color = ~pal(n_spp),
      stroke = FALSE, fillOpacity = 0.5) %>%
    addLegend("topleft", pal = pal, values = sd$data()$n_spp,
      title = "n spp", opacity = 1),
    #addMarkers(),
  datatable(sd, extensions="Scroller", style="bootstrap", class="compact", width="100%",
    options=list(deferRender=TRUE, scrollY=300, scroller=TRUE)))
```

## Relating to the Environment

- SERC Water Quality Monitoring Project for the FKNMS http://serc.fiu.edu/wqmnetwork/FKNMS-CD/DataDL.htm 

Fetched data had the following nested elements:
  * Basin name: FK= Florida Keys; SHELF= Southwest Florida Shelf (`BASIN`)
  * Biogeochemical Segment (`SEGMENT`)
  * Sampling Date (`DATE`)
  * Latitude (`LATDEC`)
  * Longitude (`LONDEC`)
  * Site Name (`SITE`)
  * Station # (`STA`)
  * Surface Temperature ᴼC (`TEMP*S`)
  * Bottom Temperature ᴼC (`TEMP-B`)


```{r SERC Water Quality}
#read SERC WQ data
WQ = read_csv('data/SERC_Water_Quality_FKNMS.csv')
View(WQ)

#summarize by SEGMENT, DATE, LATDEC, LONDEC, TEMP-S, TEMP-B
temp = WQ %>% 
    select(date = DATE, segment = SEGMENT, lat = LATDEC, long = LONDEC, surftemp = `TEMP-S`, bottemp = `TEMP-B`) %>% 
    filter(!is.na(bottemp))%>%
    filter(!is.na(surftemp))%>%
    group_by(date, lat, long, segment, surftemp, bottemp) %>%
    as.data.frame() %>%
    as.Date(date, format = '%m/%d/%y')
    
dates <- temp[,1]
(as.Date(dates, format='%m/%d/%y'))
print(dates) 
      
```

### Extract and Predict with Outside Values
- BB asked Maria for seascapes data to extract and predict with this dataset (2017-01-19)

## TODO
- Where's the Dry Tortugas data, eg for 2000?
- fix [obis_biodiv.Rmd](https://marinebon.github.io/analysis/obis_biodiv.html) with proper names
- Translate [Lab 8. Communities](http://benbestphd.com/landscape-ecology-labs/lab8.html) for calculating Bray-Curtis dissimilarity, and applying NMDS & clustering with environmental gradients (gam contour plot). 

## Notes for ERDDAP Correction
- `mapNumber` -> `mpaNumber`
- `Samples`: 361,351 records with Samples # and all other fields are NA or NaN?!
